{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_CLASSES = 10 #MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "\n",
    "    '''\n",
    "    def __init__(self,latent_dim,hidden_dim, dr=0.2, n_out=10):\n",
    "        super(ConvNetwork, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 8, kernel_size=5)\n",
    "        self.drop1 = torch.nn.Dropout2d(dr)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=5)\n",
    "        self.drop2 = torch.nn.Dropout2d(dr)\n",
    "\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(16 * 5 * 5, hidden_dim)\n",
    "        self.drop = torch.nn.Dropout(dr)\n",
    "\n",
    "        self.mu = torch.nn.Linear(hidden_dim, latent_dim, bias=False)\n",
    "        self.var = torch.nn.Linear(hidden_dim, latent_dim, bias=False)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.drop1(self.pool(self.relu(self.conv1(x))))\n",
    "        x = self.drop2(self.pool(self.relu(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.drop(self.relu(self.fc1(x)))\n",
    "        # latent parameters\n",
    "        mean = self.mu(x)\n",
    "        # mean is of shape [batch_size, latent_dim]\n",
    "        log_var = self.var(x)\n",
    "        # log_var is of shape [batch_size, latent_dim]\n",
    "\n",
    "        return mean, log_var\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "\n",
    "    '''\n",
    "    def __init__(self,latent_dim,hidden_dim, dr=0.2, n_out=10):\n",
    "        super(ConvNetwork, self).__init__()\n",
    "        self.conv1_t = torch.nn.ConvTranspose2d(8,3,kernel_size=5)\n",
    "        self.drop1 = torch.nn.Dropout2d(dr)\n",
    "        self.conv2_t = torch.nn.ConvTranspose2d(16, 8, kernel_size=5)\n",
    "        self.drop2 = torch.nn.Dropout2d(dr)\n",
    "\n",
    "        self.Unpool = torch.nn.MaxUnpool2d(2,2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.fc1_inv = torch.nn.Linear(hidden_dim, 16 * 5 * 5)\n",
    "        self.drop = torch.nn.Dropout(dr)\n",
    "\n",
    "        self.latent2hidden = torch.nn.Linear(latent_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def reverse_features(self, x):\n",
    "        x = x.reshape(16,5,5)\n",
    "        x = self.conv2_t(self.relu(self.Unpool(self.drop2(x)))) \n",
    "        x = self.conv1_t(self.relu(self.Unpool(self.drop1(x)))) \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.drop(self.latent2hidden(x))) \n",
    "        x = self.fc1_inv(x)\n",
    "        x = self.reverse_features(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(latent_dim, hidden_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, C):\n",
    "\n",
    "        x = torch.cat((x, C), dim=1)\n",
    "\n",
    "        # encode\n",
    "        z_mu, z_var = self.encoder(x)\n",
    "\n",
    "        # sample from the distribution having latent parameters z_mu, z_var\n",
    "        # reparameterize\n",
    "        std = torch.exp(z_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps.mul(std).add_(z_mu)\n",
    "\n",
    "       \n",
    "        z = torch.cat((x_sample, C2), dim=1)\n",
    "\n",
    "        # decode\n",
    "        generated_x = self.decoder(z)\n",
    "\n",
    "        return generated_x, z_mu, z_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
